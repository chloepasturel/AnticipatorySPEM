{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the template @ https://laurentperrinet.github.io/sciblog/posts/2019-09-11_video-abstract-vision.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the movie using the (*excellent*) [MoviePy](http://zulko.github.io/moviepy/index.html) library:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "BSM.mp4 --> duration: 29.92 , fps: 60.0\n",
      "eyeMvt.mp4 --> duration: 11.2 , fps: 60.0\n",
      "Bet.mp4 --> duration: 13.87 , fps: 60.0\n",
      "2_results_enregistrement.mp4 --> duration: 27.55 , fps: 60.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   0%|          | 3/8253 [00:00<05:11, 26.48it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video PasturelMontagniniPerrinet2020_video-abstract.mp4.\n",
      "Moviepy - Writing video PasturelMontagniniPerrinet2020_video-abstract.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready PasturelMontagniniPerrinet2020_video-abstract.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip, ImageClip, TextClip, CompositeVideoClip\n",
    "\n",
    "H, W = 500, 800\n",
    "H_fig, W_fig = int(H-H/(1.618*3)), int(W-W/(1.618*3))\n",
    "\n",
    "\n",
    "opt_t = dict(font=\"Open-Sans-Regular\", size=(W,H), method='caption')\n",
    "opt_st = dict(font=\"Open-Sans-SemiBold\", size=(W,H), method='caption')\n",
    "\n",
    "clip = []\n",
    "t = 0 \n",
    "\n",
    "# TITRE\n",
    "texts = [\"\"\"\n",
    "Humans adapt their anticipatory eye movements \n",
    "to the volatility of visual motion properties\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\", \"\"\"\n",
    "Humans adapt their anticipatory eye movements \n",
    "to the volatility of visual motion properties\n",
    "\n",
    "by Pasturel, Montagnini and Perrinet\n",
    "\n",
    "\n",
    "\"\"\", \"\"\"\n",
    "Humans adapt their anticipatory eye movements \n",
    "to the volatility of visual motion properties\n",
    "\n",
    "by Pasturel, Montagnini and Perrinet\n",
    "\n",
    "to appear in PLoS CB\n",
    "\"\"\"]\n",
    "txt_opts = dict(align='center', color='white', **opt_t) #stroke_color='gray', stroke_width=.5\n",
    "duration = 2.5\n",
    "for text in texts:\n",
    "    txt = TextClip(text, fontsize=35, **txt_opts).set_start(t).set_duration(duration)\n",
    "    t += duration\n",
    "    clip.append(txt)\n",
    "\n",
    "abstract = \"\"\"\n",
    "Animal behavior constantly adapts to changes, for example when the statistical properties \n",
    "of the environment change unexpectedly. For an agent that interacts with this volatile setting,\n",
    "it is important to react accurately and as quickly as possible. It has already been shown that\n",
    "when a random sequence of motion ramps of a visual target is biased to one direction (e.g. right\n",
    "or left), human observers adapt their eye movements to accurately anticipate the target's expected \n",
    "direction. Here, we prove that this ability extends to a volatile environment where the probability \n",
    "bias could change at random switching times. In addition, we also recorded the explicit prediction of \n",
    "the next outcome as reported by observers using a rating scale. Both results were compared to the estimates \n",
    "of a probabilistic agent that is optimal in relation to the assumed generative model. Compared to \n",
    "the classical leaky integrator model, we found a better match between our probabilistic agent and \n",
    "the behavioral responses, both for the anticipatory eye movements and the explicit task. Furthermore,\n",
    "by controlling the level of preference between exploitation and exploration in the model, we were able\n",
    "to fit for each individual's experimental dataset the most likely level of volatility and analyze\n",
    "inter-individual variability across participants. These results prove that in such an unstable \n",
    "environment, human observers can still represent an internal belief about the environmental \n",
    "contingencies, and use this representation both for sensory-motor control and for explicit judgments.\n",
    "This work offers an innovative approach to more generically test the diversity of human cognitive\n",
    "abilities in uncertain and dynamic environments.\n",
    "\"\"\"\n",
    "\n",
    "author_summary = \"\"\"\n",
    "Understanding how humans adapt to changing environments to make judgments or plan motor responses\n",
    "based on time-varying sensory information is crucial for psychology, neuroscience and artificial \n",
    "intelligence. Current theories for how we deal with the environment's uncertainty, that is, in \n",
    "response to the introduction of some randomness change, mostly rely on the behavior at equilibrium, \n",
    "long after after a change. Here, we show that in the more ecological case where the context switches \n",
    "at random times all along the experiment, an adaptation to this volatility can be performed online. \n",
    "In particular, we show in two behavioral experiments that humans can adapt to such volatility at the\n",
    "early sensorimotor level, through their anticipatory eye movements, but also at a higher cognitive \n",
    "level, through explicit ratings. Our results suggest that humans (and future artificial systems) can\n",
    "use much richer adaptive strategies than previously assumed.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "fig_name = {\"BSM\":['BSM.mp4'], # 30''\n",
    "            \"eyeMvt\":['eyeMvt.mp4',  # 10''\n",
    "                      # '1_B_Trace_moyenne.png',\n",
    "                      # 'demo_fit.png'\n",
    "                     ],\n",
    "            \"Bet\":['Bet.mp4'],  # 15''\n",
    "            \"BBCP\":['3_BCP_model.png', \n",
    "                    '3_BCP_readouts.png',\n",
    "                    '2_results_enregistrement.mp4', # 30''\n",
    "                    '4_A_result_psycho_aSPEM.png',\n",
    "                    '4_B_result_psycho_bet.png',\n",
    "                    '5B_inter-individual_differences_fit.png',\n",
    "                    '5A_inter-individual_differences_fit.png']}\n",
    "\n",
    "duration_dict = {'3_BCP_model.png':9, \n",
    "                 '3_BCP_readouts.png':6,\n",
    "                 '4_A_result_psycho_aSPEM.png':3,\n",
    "                 '4_B_result_psycho_bet.png':3,\n",
    "                 '5A_inter-individual_differences_fit.png':5,\n",
    "                 '5B_inter-individual_differences_fit.png':5}\n",
    "\n",
    "# INTRO\n",
    "sub_opts = dict(fontsize=28, align='center', color='white', **opt_t)\n",
    "sub_duration = 1.5\n",
    "intro_subs = [\"\"\"\n",
    "An important feature of human cognition is the capacity\n",
    "to adapt to the volatility of the environment.\n",
    "\n",
    "\n",
    "\"\"\", \"\"\"\n",
    "An important feature of human cognition is the capacity\n",
    "to adapt to the volatility of the environment.\n",
    "Indeed, it is essential to respond as fast as possible\n",
    "\n",
    "\"\"\", \"\"\"\n",
    "An important feature of human cognition is the capacity\n",
    "to adapt to the volatility of the environment.\n",
    "Indeed, it is essential to respond as fast as possible\n",
    "... but also to avoid premature decisions.\n",
    "\"\"\",\n",
    "               ]\n",
    "\n",
    "for i_sub, subtitle in enumerate(intro_subs):\n",
    "    sub = TextClip(subtitle, **sub_opts).set_start(t).set_duration(sub_duration)\n",
    "    t += sub_duration\n",
    "    clip.append(sub)    \n",
    "\n",
    "\n",
    "# LES CHOSES SERIEUSES !\n",
    "texts = [\"BSM\", \"eyeMvt\", \"Bet\", \"BBCP\"]\n",
    "colors = ['black', 'orange', 'blue', 'red']\n",
    "subtitles = {}\n",
    "subtitles['BSM.mp4'] = [\"Take for instance the case of a doctor who tests...\", \n",
    "               \"...his patients to check is some have some flu or not.\", \n",
    "               \"The underlying occurrence of these observations (Top graph)...\", \n",
    "               \"...might be caused by a switch in probability rate (Bottom)...\",\n",
    "               \"...from a normal situation (low p) to an outbreak of flu (high p).\", \n",
    "               \"In the longer-term, we may understand this volatility as...\", \n",
    "               \"...different epochs, each with a stationary rate of cases and...\", \n",
    "               \"...separated by switches which occur at random times...\", \n",
    "               \"...but with a given hazard rate (= rate of switches).\"]\n",
    "\n",
    "subtitles['eyeMvt.mp4'] = [\"How do humans behave to anticipate future outcomes?\", \n",
    "                           \"We tested this by showing trial sequences drawn from...\", \n",
    "                           \"...such a switching process. Each trial consists of \", \n",
    "                           \"...a dot appearing on the screen and moving...\", \n",
    "                           \"... either to the left or to the right.\", \n",
    "                           \"First, we recorded the eye movements of observers....\", \n",
    "                           \"...since it is known that anticipatory movements occur...\", \n",
    "                           \"...in the direction which is the most likely.\"]\n",
    "# subtitles['1_B_Trace_moyenne.png'] = ['Blabla Mvt eye... mauvaise figure...']\n",
    "# subtitles['demo_fit.png'] = ['Blabla Fit...']\n",
    "\n",
    "subtitles['Bet.mp4'] = [\"On another day, we tested the same observer...\", \n",
    "               \"...which had to explicitly adjust a cursor to guess...\", \n",
    "               \"...for the next outcome (from sure left, ..\", \n",
    "               \"...unsure, to sure right).\"]\n",
    "subtitles['3_BCP_model.png'] = [\"To understand the strategy which is used by the observers, \", \n",
    "                                \"...we created the model of an agent which represents...\", \n",
    "                                \"...different alternative beliefs of the time since a switch. \",\n",
    "                                \"We proved mathematically that this model is the best possible...\", \n",
    "                                \"...you can get knowing the switching process.\"\n",
    "                    ]\n",
    "subtitles['3_BCP_readouts.png'] = [\"We compared the results of this model (in green)...\",\n",
    "                                   \"...with a model which simply averages the last trials...\", \n",
    "                                   \"...to estimate the current bias (leaky integrator, in orange).\", \n",
    "\n",
    "                                   ]\n",
    "subtitles['2_results_enregistrement.mp4'] = [\n",
    "               \"Results shows that observers were efficient in infering...\", \n",
    "               \"...the next outcome both explicitly with the cursor (bottom)...\", \n",
    "               \"...and more surprisingly with eye movements (middle). We also...\", \n",
    "               \"...showed (top) that the model which represents switches (orange)...\", \n",
    "               \"...performed better than the leaky integrator (green),...\", \n",
    "               \"...in particular after switches.\", \n",
    "               ]\n",
    "\n",
    "subtitles['4_A_result_psycho_aSPEM.png'] = [\"This better fit demonstrates ...\", \n",
    "               \"...that humans adapt to the volatility of the environment and ...\"]\n",
    "subtitles['4_B_result_psycho_bet.png'] = [\"...have some representation for this volatility,\",\n",
    "                                         \"an adaptive strategy richer than previously assumed.\"]\n",
    "subtitles['5B_inter-individual_differences_fit.png'] = [\n",
    "                \"Moreover, adjusting the free paramater of the model...\", \n",
    "                \"...from more volatile (bottom row)...\", \n",
    "                \"...to more conservative (top row),...\", \n",
    "               \"...we could find the best fit hazard rate for each individual.\"]\n",
    "subtitles['5A_inter-individual_differences_fit.png'] = [\n",
    "               \"This shows that different individuals have different...\",\n",
    "                \"...compromise between exploration and the exploitation...\", \n",
    "               \"...a hallmark of their inter-individual differences.\"]\n",
    "\n",
    "\n",
    "# http://zulko.github.io/moviepy/ref/VideoClip/VideoClip.html?highlight=compositevideoclip#textclip\n",
    "txt_opts = dict(fontsize=65, bg_color='white', align='center', **opt_st)\n",
    "sub_opts = dict(fontsize=28, align='South', color='white', **opt_st)\n",
    "\n",
    "for text, color in zip(texts, colors):\n",
    "    # duration = 1\n",
    "    # txt = TextClip(text, color=color, **txt_opts).set_start(t).set_duration(duration)\n",
    "    # t += duration\n",
    "    # clip.append(txt)\n",
    "\n",
    "    for fig in fig_name[text] :\n",
    "    \n",
    "        if fig[-4:]=='.mp4' :\n",
    "            img = VideoFileClip('%s/%s'%(text, fig), audio=False)\n",
    "            print(fig, '--> duration:', img.duration, ', fps:', img.fps)\n",
    "            duration = img.duration\n",
    "        else :\n",
    "            duration = duration_dict[fig]\n",
    "            img = ImageClip('%s/%s'%(text, fig)).set_duration(duration)\n",
    "\n",
    "        img = img.set_start(t).set_pos('center').resize(height=H_fig, width=W_fig)\n",
    "\n",
    "        t += duration\n",
    "        clip.append(img)\n",
    "\n",
    "        # blabla\n",
    "        t_sub = t - duration\n",
    "        sub_duration = duration / len(subtitles[fig])\n",
    "        for subtitle in subtitles[fig]:\n",
    "            sub = TextClip(subtitle, **sub_opts).set_start(t_sub).set_duration(sub_duration)\n",
    "            t_sub += sub_duration\n",
    "            clip.append(sub)\n",
    "\n",
    "# OUTRO\n",
    "texts = [\"\"\"\n",
    "Overall, this study show the exquisite  \n",
    "capacities of human cognition in estimating \n",
    "the volatility of the environment and\n",
    "\n",
    "\n",
    "            \"\"\",\n",
    "         \"\"\"\n",
    "Overall, this study show the exquisite  \n",
    "capacities of human cognition in estimating \n",
    "the volatility of the environment and\n",
    "proposes methodological advances to \n",
    "quantitatively study it.\n",
    "            \"\"\"]\n",
    "\n",
    "txt_opts = dict(fontsize=30, align='center', **opt_t)\n",
    "duration = 3\n",
    "for text in texts:\n",
    "    txt = TextClip(text, color='white', **txt_opts).set_start(t).set_duration(duration)\n",
    "    t += duration\n",
    "    clip.append(txt)\n",
    "    \n",
    "# FIN\n",
    "texts = [\n",
    "         \"For more info,\\n and the full, open-sourced code\\n visit \", \n",
    "         \"https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020\"]\n",
    "\n",
    "txt_opts = dict(align='center', **opt_t)\n",
    "duration = 3\n",
    "for text, fontsize in zip(texts, [30, 24]):\n",
    "    txt = TextClip(text, color='orange', fontsize=fontsize, **txt_opts).set_start(t).set_duration(duration)\n",
    "    t += duration\n",
    "    clip.append(txt)    \n",
    "\n",
    "video = CompositeVideoClip(clip)\n",
    "video.write_videofile('PasturelMontagniniPerrinet2020_video-abstract.mp4', fps=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<BR>\n",
    "<center><video controls autoplay loop src=\"PasturelMontagniniPerrinet2020_video-abstract.mp4\" width=61.8%/></a> </center>\n",
    "<BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 2c40501]  Update 2020-03-24_video-abstract.ipynb\n",
      " 44 files changed, 12 insertions(+), 40 deletions(-)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_0.png (96%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_1.png (76%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_10.png (71%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_11.png (93%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_12.png (74%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_13.png (96%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_14.png (74%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_15.png (98%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_16.png (74%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_17.png (85%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_18.png (75%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_19.png (92%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_2.png (79%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_20.png (77%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_21.png (98%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_22.png (97%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_23.png (71%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_24.png (70%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_25.png (70%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_26.png (70%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_27.png (71%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_28.png (72%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_29.png (70%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_3.png (92%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_30.png (75%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_31.png (74%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_32.png (75%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_33.png (76%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_34.png (75%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_35.png (75%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_36.png (76%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_37.png (75%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_38.png (76%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_39.png (75%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_4.png (71%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_5.png (96%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_6.png (73%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_7.png (84%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_8.png (71%)\n",
      " rewrite 2020-03_video-abstract/BSM/proba_fig/proba_bsm_9.png (91%)\n",
      "Enumerating objects: 101, done.\n",
      "Counting objects: 100% (101/101), done.\n",
      "Delta compression using up to 4 threads\n",
      "Compressing objects: 100% (53/53), done.\n",
      "Writing objects: 100% (53/53), 253.33 KiB | 6.50 MiB/s, done.\n",
      "Total 53 (delta 10), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (10/10), completed with 7 local objects.\u001b[K\n",
      "To https://github.com/chloepasturel/AnticipatorySPEM\n",
      "   1f1a62d..2c40501  master -> master\n"
     ]
    }
   ],
   "source": [
    "!git commit -am' Update 2020-03-24_video-abstract.ipynb ' ;  git push"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
