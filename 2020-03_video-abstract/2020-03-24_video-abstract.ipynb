{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the template @ https://laurentperrinet.github.io/sciblog/posts/2019-09-11_video-abstract-vision.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: figures/: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%ls figures/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TextClip.list('font')\n",
    "'''\n",
    "'Open-Sans-Bold',\n",
    " 'Open-Sans-Bold-Italic',\n",
    " 'Open-Sans-ExtraBold',\n",
    " 'Open-Sans-ExtraBold-Italic',\n",
    " 'Open-Sans-Italic',\n",
    " 'Open-Sans-Light',\n",
    " 'Open-Sans-Light-Italic',\n",
    " 'Open-Sans-Regular',\n",
    " 'Open-Sans-SemiBold',\n",
    " 'Open-Sans-SemiBold-Italic'\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the movie using the (*excellent*) [MoviePy](http://zulko.github.io/moviepy/index.html) library:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "BSM.mp4 --> 29.92 60.0\n",
      "eyeMvt.mp4 --> 11.2 60.0\n",
      "Bet.mp4 --> 13.87 60.0\n",
      "2_results_enregistrement.mp4 --> 27.55 60.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   0%|          | 3/7503 [00:00<04:35, 27.19it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video PasturelMontagniniPerrinet2020_video-abstract.mp4.\n",
      "Moviepy - Writing video PasturelMontagniniPerrinet2020_video-abstract.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready PasturelMontagniniPerrinet2020_video-abstract.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip, ImageClip, TextClip, CompositeVideoClip\n",
    "\n",
    "\n",
    "fig_name = {\"BSM\":['BSM.mp4'],\n",
    "            \"eyeMvt\":['eyeMvt.mp4',\n",
    "                      '1_B_Trace_moyenne.png',\n",
    "                      'demo_fit.png'],\n",
    "            \"Bet\":['Bet.mp4',\n",
    "                   '2_results_enregistrement.mp4'],\n",
    "            \"BBCP\":['3_BCP_model.png',\n",
    "                    '3_BCP_readouts.png',\n",
    "                    '4_A_result_psycho_aSPEM.png',\n",
    "                    '4_B_result_psycho_bet.png',\n",
    "                    '5A_inter-individual_differences_fit.png',\n",
    "                    '5B_inter-individual_differences_fit.png']}\n",
    "H, W = 500, 800\n",
    "H_fig, W_fig = int(H-H/(1.618*3)), int(W-W/(1.618*3))\n",
    "\n",
    "\n",
    "opt_t = dict(font=\"Open-Sans-Regular\", size=(W,H), method='caption')\n",
    "opt_st = dict(font=\"Open-Sans-SemiBold\", size=(W,H), method='caption')\n",
    "\n",
    "clip = []\n",
    "t = 0 \n",
    "\n",
    "# TITRE\n",
    "texts = [\"Humans adapt their anticipatory eye movements to the volatility of visual motion properties\", \n",
    "         'to appear in PLoS CB']\n",
    "txt_opts = dict(align='center', color='white', **opt_t) #stroke_color='gray', stroke_width=.5\n",
    "duration = 2\n",
    "for text, size in zip(texts, [50, 25]):\n",
    "    txt = TextClip(text, fontsize=size, **txt_opts).set_start(t).set_duration(duration)\n",
    "    t += duration\n",
    "    clip.append(txt)\n",
    "\n",
    "abstract = \"\"\"\n",
    "Animal behavior constantly adapts to changes, for example when the statistical properties \n",
    "of the environment change unexpectedly. For an agent that interacts with this volatile setting,\n",
    "it is important to react accurately and as quickly as possible. It has already been shown that\n",
    "when a random sequence of motion ramps of a visual target is biased to one direction (e.g. right\n",
    "or left), human observers adapt their eye movements to accurately anticipate the target's expected \n",
    "direction. Here, we prove that this ability extends to a volatile environment where the probability \n",
    "bias could change at random switching times. In addition, we also recorded the explicit prediction of \n",
    "the next outcome as reported by observers using a rating scale. Both results were compared to the estimates \n",
    "of a probabilistic agent that is optimal in relation to the assumed generative model. Compared to \n",
    "the classical leaky integrator model, we found a better match between our probabilistic agent and \n",
    "the behavioral responses, both for the anticipatory eye movements and the explicit task. Furthermore,\n",
    "by controlling the level of preference between exploitation and exploration in the model, we were able\n",
    "to fit for each individual's experimental dataset the most likely level of volatility and analyze\n",
    "inter-individual variability across participants. These results prove that in such an unstable \n",
    "environment, human observers can still represent an internal belief about the environmental \n",
    "contingencies, and use this representation both for sensory-motor control and for explicit judgments.\n",
    "This work offers an innovative approach to more generically test the diversity of human cognitive\n",
    "abilities in uncertain and dynamic environments.\n",
    "\"\"\"\n",
    "\n",
    "author_summary = \"\"\"\n",
    "Understanding how humans adapt to changing environments to make judgments or plan motor responses\n",
    "based on time-varying sensory information is crucial for psychology, neuroscience and artificial \n",
    "intelligence. Current theories for how we deal with the environment's uncertainty, that is, in \n",
    "response to the introduction of some randomness change, mostly rely on the behavior at equilibrium, \n",
    "long after after a change. Here, we show that in the more ecological case where the context switches \n",
    "at random times all along the experiment, an adaptation to this volatility can be performed online. \n",
    "In particular, we show in two behavioral experiments that humans can adapt to such volatility at the\n",
    "early sensorimotor level, through their anticipatory eye movements, but also at a higher cognitive \n",
    "level, through explicit ratings. Our results suggest that humans (and future artificial systems) can\n",
    "use much richer adaptive strategies than previously assumed.\n",
    "\"\"\"\n",
    "%: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "intro_subs =  [\"\"\"\n",
    "An important feature of human cognition is a capacity to adapt to the volatility of the environment tired dad\n",
    "Indeed, it is essential to respond as fast as possible to a change, but also to do this with the best decision to avoid wrong decisions\n",
    "\"\"\"\n",
    ",\"\"\"\n",
    "Take for instants the keys off adductor would test is patient test is patience the check is some have the flu :-) all not nuts made it patio\n",
    "\"\"\"\n",
    ",\"\"\"\n",
    "\n",
    "Three\n",
    "The underlying cause of this observation might be as a switch from a normal situation to an outbreak of flu.\n",
    "At the longer-term, we may understand his volatility is different at pucks who is a stationary rate of cases and separated by switches which occur at from the room￼￼￼￼￼ which occur at random times with batteries a given has a great hazard hazard rate rate.\n",
    "\"\"\"\n",
    ",\"\"\"\n",
    "\n",
    "Five\n",
    "How do humans behave to best anticipate future outcomes? We tested this by showing a sequence of trials grown from such a switching process. Each trial consists of a dotappearing on the screen and moving eyes are to the left or to the right dot.\n",
    "First, we are recorded the eye movements of observers.indeed it is known that anticipatory movements occur in the direction which is the most likely.\n",
    "\"\"\"\n",
    ",\"\"\"\n",
    "\n",
    "Six\n",
    "On another day we tested the same observer which had to explicitly adjust a cursor to get a follow the next outcome\n",
    "To guess for the next outcome\n",
    "\"\"\"\n",
    ",\"\"\"\n",
    "\n",
    "Seven\n",
    "To understand the strategy which is used by the observers, we compared his results with two different mothers current one which is simply average is obvious the last trials to estimate the current buyers\n",
    "The Lecuit integrator\n",
    "And another model which hasn’t been done on the presentation of different alternative beliefs of the time since I switch and which is the best mother you can derive from the switching process mathematically\n",
    "\"\"\"\n",
    ",\"\"\"\n",
    "\n",
    "Eight\n",
    "Results shows that observers were efficient in selecting the next outcome both explicitly and more surprisingly also was there a movements we should also that the model which hoppers and switches performed better than the leaky integrator.\n",
    "Impetigo down after switches\n",
    "This is a better fit demonstrates that humans adapt to the volatility of the envelope environment and know about it and have a presentation for his volatility\n",
    "\"\"\"\n",
    ",\"\"\"\n",
    "\n",
    "Nine\n",
    "Moreover, by adjusting the free pie matter of the model we could find the best fit has that rate for each individual that she’s showing Patti Klaus a different individuals have different, best compromise between explanation and the expectation which should be maybe a hallmark of their intent of the individual differences\n",
    "\n",
    "Overall, this study put the exquisite capacities of human cognition in estimating the volatility of the environment and poses submitted a logical advances to contact actively assess them\n",
    "\"\"\"\n",
    "]\n",
    "# INTRO\n",
    "sub_opts = dict(fontsize=32, align='center', color='white', **opt_t)\n",
    "sub_duration = 1.5\n",
    "for subtitle in intro_subs:\n",
    "    sub = TextClip(subtitle, **sub_opts).set_start(t).set_duration(sub_duration)\n",
    "    t += sub_duration\n",
    "    clip.append(sub)    \n",
    "\n",
    "# LES CHOSES SERIEUSES !\n",
    "texts = [\"BSM\", \"eyeMvt\", \"Bet\", \"BBCP\"]\n",
    "colors = ['black', 'orange', 'blue', 'red']\n",
    "subtitles = {}\n",
    "subtitles['BSM.mp4'] = ['Blabla bsm...', '... blablabla', '... et encore blabla...']\n",
    "\n",
    "subtitles['eyeMvt.mp4'] = ['Blabla eyeMvt...', '... blablabla', '... et encore blabla...']\n",
    "subtitles['1_B_Trace_moyenne.png'] = ['Blabla Mvt eye... mauvaise figure...']\n",
    "subtitles['demo_fit.png'] = ['Blabla Fit...']\n",
    "\n",
    "subtitles['Bet.mp4'] = ['Blabla bet...', '... blablabla', '... et encore blabla...']\n",
    "subtitles['2_results_enregistrement.mp4'] = ['Blabla...']\n",
    "\n",
    "subtitles['3_BCP_model.png'] = ['Blabla...']\n",
    "subtitles['3_BCP_readouts.png'] = ['Blabla...']\n",
    "subtitles['4_A_result_psycho_aSPEM.png'] = ['Blabla...']\n",
    "subtitles['4_B_result_psycho_bet.png'] = ['Blabla...']\n",
    "subtitles['5A_inter-individual_differences_fit.png'] = ['Blabla...']\n",
    "subtitles['5B_inter-individual_differences_fit.png'] = ['Blabla...']\n",
    "\n",
    "\n",
    "# http://zulko.github.io/moviepy/ref/VideoClip/VideoClip.html?highlight=compositevideoclip#textclip\n",
    "txt_opts = dict(fontsize=65, bg_color='white', align='center', **opt_st)\n",
    "sub_opts = dict(fontsize=32, align='South', color='white', **opt_st)\n",
    "\n",
    "for text, color in zip(texts, colors):\n",
    "    duration = 1\n",
    "    txt = TextClip(text, color=color, **txt_opts).set_start(t).set_duration(duration)\n",
    "    t += duration\n",
    "    clip.append(txt)\n",
    "\n",
    "    for fig in fig_name[text] :\n",
    "    \n",
    "        if fig[-4:]=='.mp4' :\n",
    "            img = VideoFileClip('%s/%s'%(text, fig), audio=False)\n",
    "            print(fig, '-->', img.duration, img.fps)\n",
    "            duration = img.duration\n",
    "        else :\n",
    "            duration = 3\n",
    "            img = ImageClip('%s/%s'%(text, fig)).set_duration(duration)\n",
    "\n",
    "        img = img.set_start(t).set_pos('center').resize(height=H_fig, width=W_fig)\n",
    "\n",
    "        t += duration\n",
    "        clip.append(img)\n",
    "\n",
    "        # blabla\n",
    "        t_sub = t - duration\n",
    "        sub_duration = duration / len(subtitles[fig])\n",
    "        for subtitle in subtitles[fig]:\n",
    "            sub = TextClip(subtitle, **sub_opts).set_start(t_sub).set_duration(sub_duration)\n",
    "            t_sub += sub_duration\n",
    "            clip.append(sub)\n",
    "\n",
    "# FIN\n",
    "texts = [\"... for more info,\\n and the full, open-sourced code\\n visit \", \"https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020\"]\n",
    "colors_outro = ['orange', 'white']\n",
    "\n",
    "txt_opts = dict(fontsize=30, align='center', **opt_t)\n",
    "duration = 3\n",
    "for text, color in zip(texts, colors_outro):\n",
    "    txt = TextClip(text, color=color, **txt_opts).set_start(t).set_duration(duration)\n",
    "    t += duration\n",
    "    clip.append(txt)\n",
    "      \n",
    "    \n",
    "video = CompositeVideoClip(clip)\n",
    "video.write_videofile('PasturelMontagniniPerrinet2020_video-abstract.mp4', fps=60) #fps=60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<BR>\n",
    "<center><video controls autoplay loop src=\"figures/video/video_test.mp4\" width=61.8%/></a> </center>\n",
    "<BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
