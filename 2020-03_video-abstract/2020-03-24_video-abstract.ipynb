{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the template @ https://laurentperrinet.github.io/sciblog/posts/2019-09-11_video-abstract-vision.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: figures/: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%ls figures/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TextClip.list('font')\n",
    "'''\n",
    "'Open-Sans-Bold',\n",
    " 'Open-Sans-Bold-Italic',\n",
    " 'Open-Sans-ExtraBold',\n",
    " 'Open-Sans-ExtraBold-Italic',\n",
    " 'Open-Sans-Italic',\n",
    " 'Open-Sans-Light',\n",
    " 'Open-Sans-Light-Italic',\n",
    " 'Open-Sans-Regular',\n",
    " 'Open-Sans-SemiBold',\n",
    " 'Open-Sans-SemiBold-Italic'\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the movie using the (*excellent*) [MoviePy](http://zulko.github.io/moviepy/index.html) library:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "BSM.mp4 --> 29.92 60.0\n",
      "eyeMvt.mp4 --> 11.2 60.0\n",
      "Bet.mp4 --> 13.87 60.0\n",
      "2_results_enregistrement.mp4 --> 27.55 60.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   0%|          | 3/7503 [00:00<04:35, 27.19it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video PasturelMontagniniPerrinet2020_video-abstract.mp4.\n",
      "Moviepy - Writing video PasturelMontagniniPerrinet2020_video-abstract.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready PasturelMontagniniPerrinet2020_video-abstract.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip, ImageClip, TextClip, CompositeVideoClip\n",
    "\n",
    "\n",
    "fig_name = {\"BSM\":['BSM.mp4'],\n",
    "            \"eyeMvt\":['eyeMvt.mp4',\n",
    "                      '1_B_Trace_moyenne.png',\n",
    "                      'demo_fit.png'],\n",
    "            \"Bet\":['Bet.mp4',\n",
    "                   '2_results_enregistrement.mp4'],\n",
    "            \"BBCP\":['3_BCP_model.png',\n",
    "                    '3_BCP_readouts.png',\n",
    "                    '4_A_result_psycho_aSPEM.png',\n",
    "                    '4_B_result_psycho_bet.png',\n",
    "                    '5A_inter-individual_differences_fit.png',\n",
    "                    '5B_inter-individual_differences_fit.png']}\n",
    "H, W = 500, 800\n",
    "H_fig, W_fig = int(H-H/(1.618*3)), int(W-W/(1.618*3))\n",
    "\n",
    "\n",
    "opt_t = dict(font=\"Open-Sans-Regular\", size=(W,H), method='caption')\n",
    "opt_st = dict(font=\"Open-Sans-SemiBold\", size=(W,H), method='caption')\n",
    "\n",
    "clip = []\n",
    "t = 0 \n",
    "\n",
    "# TITRE\n",
    "texts = [\"Humans adapt their anticipatory eye movements to the volatility of visual motion properties\", \n",
    "         'to appear in PLoS CB']\n",
    "txt_opts = dict(align='center', color='white', **opt_t) #stroke_color='gray', stroke_width=.5\n",
    "duration = 2\n",
    "for text, size in zip(texts, [50, 25]):\n",
    "    txt = TextClip(text, fontsize=size, **txt_opts).set_start(t).set_duration(duration)\n",
    "    t += duration\n",
    "    clip.append(txt)\n",
    "\n",
    "abstract = \"\"\"\n",
    "Animal behavior constantly adapts to changes, for example when the statistical properties \n",
    "of the environment change unexpectedly. For an agent that interacts with this volatile setting,\n",
    "it is important to react accurately and as quickly as possible. It has already been shown that\n",
    "when a random sequence of motion ramps of a visual target is biased to one direction (e.g. right\n",
    "or left), human observers adapt their eye movements to accurately anticipate the target's expected \n",
    "direction. Here, we prove that this ability extends to a volatile environment where the probability \n",
    "bias could change at random switching times. In addition, we also recorded the explicit prediction of \n",
    "the next outcome as reported by observers using a rating scale. Both results were compared to the estimates \n",
    "of a probabilistic agent that is optimal in relation to the assumed generative model. Compared to \n",
    "the classical leaky integrator model, we found a better match between our probabilistic agent and \n",
    "the behavioral responses, both for the anticipatory eye movements and the explicit task. Furthermore,\n",
    "by controlling the level of preference between exploitation and exploration in the model, we were able\n",
    "to fit for each individual's experimental dataset the most likely level of volatility and analyze\n",
    "inter-individual variability across participants. These results prove that in such an unstable \n",
    "environment, human observers can still represent an internal belief about the environmental \n",
    "contingencies, and use this representation both for sensory-motor control and for explicit judgments.\n",
    "This work offers an innovative approach to more generically test the diversity of human cognitive\n",
    "abilities in uncertain and dynamic environments.\n",
    "\"\"\"\n",
    "\n",
    "author_summary = \"\"\"\n",
    "Understanding how humans adapt to changing environments to make judgments or plan motor responses\n",
    "based on time-varying sensory information is crucial for psychology, neuroscience and artificial \n",
    "intelligence. Current theories for how we deal with the environment's uncertainty, that is, in \n",
    "response to the introduction of some randomness change, mostly rely on the behavior at equilibrium, \n",
    "long after after a change. Here, we show that in the more ecological case where the context switches \n",
    "at random times all along the experiment, an adaptation to this volatility can be performed online. \n",
    "In particular, we show in two behavioral experiments that humans can adapt to such volatility at the\n",
    "early sensorimotor level, through their anticipatory eye movements, but also at a higher cognitive \n",
    "level, through explicit ratings. Our results suggest that humans (and future artificial systems) can\n",
    "use much richer adaptive strategies than previously assumed.\n",
    "\"\"\"\n",
    "%: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "intro_subs =  [\"Blabla intro papier... \", \"... blablabla ...\", \"... et encore blablabla\"]\n",
    "# INTRO\n",
    "sub_opts = dict(fontsize=32, align='center', color='white', **opt_t)\n",
    "sub_duration = 1.5\n",
    "for subtitle in intro_subs:\n",
    "    sub = TextClip(subtitle, **sub_opts).set_start(t).set_duration(sub_duration)\n",
    "    t += sub_duration\n",
    "    clip.append(sub)    \n",
    "\n",
    "# LES CHOSES SERIEUSES !\n",
    "texts = [\"BSM\", \"eyeMvt\", \"Bet\", \"BBCP\"]\n",
    "colors = ['black', 'orange', 'blue', 'red']\n",
    "subtitles = {}\n",
    "subtitles['BSM.mp4'] = ['Blabla bsm...', '... blablabla', '... et encore blabla...']\n",
    "\n",
    "subtitles['eyeMvt.mp4'] = ['Blabla eyeMvt...', '... blablabla', '... et encore blabla...']\n",
    "subtitles['1_B_Trace_moyenne.png'] = ['Blabla Mvt eye... mauvaise figure...']\n",
    "subtitles['demo_fit.png'] = ['Blabla Fit...']\n",
    "\n",
    "subtitles['Bet.mp4'] = ['Blabla bet...', '... blablabla', '... et encore blabla...']\n",
    "subtitles['2_results_enregistrement.mp4'] = ['Blabla...']\n",
    "\n",
    "subtitles['3_BCP_model.png'] = ['Blabla...']\n",
    "subtitles['3_BCP_readouts.png'] = ['Blabla...']\n",
    "subtitles['4_A_result_psycho_aSPEM.png'] = ['Blabla...']\n",
    "subtitles['4_B_result_psycho_bet.png'] = ['Blabla...']\n",
    "subtitles['5A_inter-individual_differences_fit.png'] = ['Blabla...']\n",
    "subtitles['5B_inter-individual_differences_fit.png'] = ['Blabla...']\n",
    "\n",
    "\n",
    "# http://zulko.github.io/moviepy/ref/VideoClip/VideoClip.html?highlight=compositevideoclip#textclip\n",
    "txt_opts = dict(fontsize=65, bg_color='white', align='center', **opt_st)\n",
    "sub_opts = dict(fontsize=32, align='South', color='white', **opt_st)\n",
    "\n",
    "for text, color in zip(texts, colors):\n",
    "    duration = 1\n",
    "    txt = TextClip(text, color=color, **txt_opts).set_start(t).set_duration(duration)\n",
    "    t += duration\n",
    "    clip.append(txt)\n",
    "\n",
    "    for fig in fig_name[text] :\n",
    "    \n",
    "        if fig[-4:]=='.mp4' :\n",
    "            img = VideoFileClip('%s/%s'%(text, fig), audio=False)\n",
    "            print(fig, '-->', img.duration, img.fps)\n",
    "            duration = img.duration\n",
    "        else :\n",
    "            duration = 3\n",
    "            img = ImageClip('%s/%s'%(text, fig)).set_duration(duration)\n",
    "\n",
    "        img = img.set_start(t).set_pos('center').resize(height=H_fig, width=W_fig)\n",
    "\n",
    "        t += duration\n",
    "        clip.append(img)\n",
    "\n",
    "        # blabla\n",
    "        t_sub = t - duration\n",
    "        sub_duration = duration / len(subtitles[fig])\n",
    "        for subtitle in subtitles[fig]:\n",
    "            sub = TextClip(subtitle, **sub_opts).set_start(t_sub).set_duration(sub_duration)\n",
    "            t_sub += sub_duration\n",
    "            clip.append(sub)\n",
    "\n",
    "# FIN\n",
    "texts = [\"... for more info,\\n and open-sourced code\\n visit \", \"lien_publi\"]\n",
    "colors_outro = ['orange', 'white']\n",
    "\n",
    "txt_opts = dict(fontsize=30, align='center', **opt_t)\n",
    "duration = 3\n",
    "for text, color in zip(texts, colors_outro):\n",
    "    txt = TextClip(text, color=color, **txt_opts).set_start(t).set_duration(duration)\n",
    "    t += duration\n",
    "    clip.append(txt)\n",
    "      \n",
    "    \n",
    "video = CompositeVideoClip(clip)\n",
    "video.write_videofile('PasturelMontagniniPerrinet2020_video-abstract.mp4', fps=60) #fps=60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<BR>\n",
    "<center><video controls autoplay loop src=\"figures/video/video_test.mp4\" width=61.8%/></a> </center>\n",
    "<BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
